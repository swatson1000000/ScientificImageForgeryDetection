# Forgery Detection Improvement Plan

## üèÜ CURRENT BEST (December 19, 2025)

### Best Configuration: Two-Stage Pipeline (Classifier + 4-Model Ensemble)

**Scripts**: 
- `bin/script_3_two_stage_submission.py` - Generate submissions
- `bin/validate_test.py` - Validate on test sets

**Test Set Results (500 forged, 500 authentic):**

| Metric | Value |
|--------|-------|
| **Forged Recall** | **78.8%** (394/500) |
| **Authentic FP** | **3.4%** (17/500) |
| **Net Score** | **+377** |
| **Precision** | **95.9%** |

**Two-Stage Pipeline:**
1. **Stage 1**: Binary classifier filters likely authentic images (threshold=0.25)
2. **Stage 2**: 4-model V4 ensemble with TTA generates masks for remaining images

**‚ö†Ô∏è CRITICAL Preprocessing Requirements (see CLAUDE.md):**
1. **ImageNet normalization** - `(img - [0.485, 0.456, 0.406]) / [0.229, 0.224, 0.225]`
2. **Adaptive threshold** - brightness-based (50/80/200 scale)
3. **Connected components filtering** - remove small noise regions

**Optimal Configuration:**

| Parameter | Value | Description |
|-----------|-------|-------------|
| classifier_threshold | **0.25** | Binary classifier filter threshold |
| seg_threshold | **0.35** | Segmentation detection threshold |
| min_area | **300** | Minimum forgery area |
| TTA | 4x | Flip augmentations (horizontal, vertical, both) |
| adaptive | True | Lower threshold for dark images |
| ensemble | 4 models | Mean aggregation of predictions |

**Models Used:**
- `binary_classifier_best.pth` - Binary classifier (82.8% accuracy, 86.5% recall)
- `highres_no_ela_v4_best.pth` - V4 primary model
- `hard_negative_v4_best.pth` - Hard negative trained
- `high_recall_v4_best.pth` - High recall variant
- `enhanced_aug_v4_best.pth` - Enhanced augmentation

**Usage:**
```bash
# Validation on test sets
python bin/validate_test.py --classifier-threshold 0.25 --seg-threshold 0.35 --min-area 300

# Generate submission
python bin/script_3_two_stage_submission.py -i <test_dir> -o submission.csv
```

**Comparison:**

| Config | TP | FP | Net | Recall | FP Rate |
|--------|-----|-----|------|--------|---------|
| Without ImageNet norm | 310 | 37 | 273 | 62.0% | 7.4% |
| **With all fixes** | **394** | **17** | **377** | **78.8%** | **3.4%** |

---

## Previous Best (December 17, 2025)

### Full Dataset Results (2751 forged, 2377 authentic)

| Metric | Value |
|--------|-------|
| **Forged Recall** | **79.0%** (2173/2751) |
| **Authentic FP** | **5.9%** (140/2377) |
| **Net Score** | **+2033** |

---

## Previous Best (December 14, 2025)

### V4 Ensemble + Optimized Parameters (Single-Stage)

**Script**: `bin_orig/script_2_generate_submission.py`

**Full Dataset Results (2751 forged, 2377 authentic):**

| Metric | Value |
|--------|-------|
| **Forged Recall** | **82.3%** (2265/2751) |
| **Authentic FP** | **8.3%** (198/2377) |
| **Net Score** | **+2067** |

**Optimal Configuration:**

| Parameter | Value | Description |
|-----------|-------|-------------|
| threshold | **0.35** | Detection threshold (was 0.30) |
| min_area | **300** | Minimum forgery area (was 500) |
| TTA | 4x | Flip augmentations (horizontal, vertical, both) |
| adaptive | True | Lower threshold/min_area for dark images |
| min_confidence | 0.45 | Reject low-confidence detections |

---

## Previous Best (December 13, 2025)

#### Available Modes

| Mode | Threshold | Recall | Auth FP | Precision | Use Case |
|------|-----------|--------|---------|-----------|----------|
| **high-precision** | 0.8 | 27% | **0/100** | **100%** | Automated flagging, no false alarms |
| **balanced** | 0.75 | 33% | 3/100 | 91.7% | General screening |
| **high-recall** | 0.6 | 62% | 33/100 | 65.3% | Comprehensive detection |

#### Model Ensemble Components (4 models)
| Model | Weight | Channels | Val MCC | Purpose |
|-------|--------|----------|---------|---------|
| highres_no_ela_best.pth | 1.5 | RGB | 0.5887 | Best single model |
| hard_negative_best.pth | 1.2 | RGB | 0.5207 | Reduces false positives |
| enhanced_aug_best.pth | 1.0 | RGB | 0.5664 | Copy-paste augmented |
| comprehensive_best.pth | 0.5 | RGB+ELA | 0.4795 | ELA features |

### Key Achievement: Zero False Positives Mode
At threshold 0.8, the 4-model ensemble achieves **100% precision** with **zero false positives** on the test set of 100 authentic images.

---

### ü•à Previous Best: 3-Model Ensemble @ Threshold 0.8
| Metric | Value | Notes |
|--------|-------|-------|
| Recall | 50% | 50 of 100 forgeries detected |
| Precision | 73.5% | 18% false positive rate |
| Auth False Positives | 18/100 | |

### ü•â Best Single Model: High-res 512x512 No-ELA
| Metric | Value | Threshold |
|--------|-------|-----------|
| Recall | **57.94%** | 0.5 |
| Precision | **62.21%** | 0.5 |
| F1 Score | **0.6000** | 0.5 |
| Pixel MCC | **0.5887** | 0.5 |
| Auth False Positives | 67/100 | 0.5 |

At threshold 0.9: Precision 76.8%, Recall 50.4%, MCC 0.6130

### Model Comparison (Test Set - 100 forged + 100 authentic)
| Model | Resolution | Recall | Precision | F1 | MCC |
|-------|------------|--------|-----------|-----|-----|
| **High-res No-ELA** | 512x512 | **0.5794** | 0.6221 | **0.6000** | **0.5887** |
| High-res 768 | 768x768 | 0.3850 | 0.6492 | 0.4833 | 0.4887 |
| Comprehensive (ELA) | 512x512 | 0.4495 | 0.5416 | 0.4913 | 0.4795 |
| SAFIRE (pretrained) | 1024x1024 | 0.3476 | 0.0645 | 0.1089 | - |

### Key Insights
1. **ELA hurts performance** on PNG images (no JPEG compression artifacts)
2. **512x512 is optimal** - 768 reduces recall significantly
3. **Domain-specific training essential** - SAFIRE (general) fails on scientific images

---

## Original Baseline (for reference)
| Metric | Value |
|--------|-------|
| Recall (Forged Detection) | 66% |
| Specificity (Authentic Detection) | 97% |
| Precision | 95.65% |
| MCC | 0.663 |
| Accuracy | 81.5% |

**Key Finding**: Missed forgeries are predominantly small (median 0.95% of image area).

---

## Phase 7: Post-Processing to Reduce False Positives (December 8, 2025)

### 7.1 Analysis
False positives on authentic images are often small regions (0.07% - 6% of image area).

### 7.2 Strategies Tested

| Strategy | Forged Det | Auth FP | Recall | Precision | F1 | MCC |
|----------|------------|---------|--------|-----------|-----|-----|
| No post-proc (baseline) | 76/100 | 67/100 | 0.5794 | 0.6221 | 0.6000 | 0.5887 |
| **Thresh 0.9** | 69/100 | **49**/100 | 0.5036 | **0.7680** | **0.6083** | **0.6130** |
| MinArea 500 | 64/100 | 46/100 | 0.5779 | 0.6238 | 0.6000 | 0.5888 |
| Thresh0.8+MinArea100 | 69/100 | 48/100 | 0.5311 | 0.7061 | 0.6062 | 0.6024 |

### 7.3 Best Strategy: Threshold 0.9
- Reduces auth false positives by **27%** (67‚Üí49)
- Improves MCC from 0.5887 ‚Üí **0.6130**
- Precision jumps from 62% ‚Üí **77%**
- Slight recall drop (58% ‚Üí 50%) - acceptable tradeoff

### 7.4 Recommended Inference Settings
```python
THRESHOLD = 0.9  # Higher threshold for fewer false positives
MIN_AREA = 100   # Optional: remove tiny detections
```

### 7.5 Full Validation Set Results (934 images)
**Script**: `bin_4/script_7_optimized_inference.py`

| Threshold | MinArea | Detections | Recall | Precision | F1 | MCC | IoU |
|-----------|---------|------------|--------|-----------|-----|-----|-----|
| 0.5 | 0 | 773/934 | 0.4873 | 0.6024 | 0.5388 | 0.5269 | 0.3687 |
| **0.9** | **100** | 672/934 | 0.4261 | **0.7015** | 0.5302 | **0.5344** | 0.3607 |

**Conclusions on Full Validation**:
- Threshold 0.9 maintains better precision (70% vs 60%)
- F1 scores are similar (~0.53)
- MCC slightly better at 0.9 (0.5344 vs 0.5269)
- Inference speed: ~32 images/second

---

## Phase 8: Ensemble Methods (December 8, 2025) ‚úÖ COMPLETED

### 8.1 Models in Ensemble
| Model | File | Channels | Weight | MCC |
|-------|------|----------|--------|-----|
| High-res No-ELA | `highres_no_ela_best.pth` | 3 (RGB) | 1.5 | 0.5887 |
| Enhanced Aug | `enhanced_aug_best.pth` | 3 (RGB) | 1.0 | 0.5664 |
| Comprehensive (ELA) | `comprehensive_best.pth` | 4 (RGB+ELA) | 0.7 | 0.4795 |

### 8.2 Ensemble Results (3-Model, weighted_avg)

| Threshold | Forged Detected | Auth FP | Precision | Notes |
|-----------|-----------------|---------|-----------|-------|
| 0.70 | 62/100 | 33/100 | 65.3% | High recall |
| 0.75 | 60/100 | 30/100 | 66.7% | |
| **0.80** | **50/100** | **18/100** | **73.5%** | ‚≠ê Best balanced |
| 0.85 | 36/100 | 10/100 | 78.3% | High precision |
| 0.90 | 20/100 | 3/100 | 87.0% | Very conservative |

### 8.3 Comparison: Ensemble vs Single Model

| Configuration | Recall | Auth FP | Precision |
|---------------|--------|---------|-----------|
| Single model, t=0.5 | 58% | 67% | 46% |
| Single model, t=0.9 | 50% | 49% | 50% |
| **3-model ensemble, t=0.8** | **50%** | **18%** | **73.5%** |

**Key Finding**: The 3-model ensemble at threshold 0.8 achieves:
- **63% reduction in false positives** (49 ‚Üí 18) vs single model at t=0.9
- **Same recall** (50%) as single model at t=0.9
- **+23.5% precision gain** (50% ‚Üí 73.5%)

### 8.4 Recommended Configurations

**For high precision (minimal false alarms)**:
```bash
python bin_4/script_8_ensemble_inference.py --image_folder <path> --threshold 0.8
```
- 73.5% precision, 50% recall, 18% FP rate

**For high recall (catch most forgeries)**:
```bash
python bin_4/script_8_ensemble_inference.py --image_folder <path> --threshold 0.7
```
- 65% precision, 62% recall, 33% FP rate

### 8.5 Enhanced Augmentation Model Training
**Script**: `bin_4/script_4_enhanced_augmentation.py`
- Copy-paste augmentation (195 forgery patches extracted)
- Stronger color/noise augmentation
- Best Val MCC: 0.5664 (epoch 20)

---

## Phase 9: Architecture Experiments ‚úÖ COMPLETED

### 9.1 EfficientNet-B4 (Larger Backbone)
**Script**: `bin_4/script_4_efficientnet_b4.py`
- Parameters: 19.4M (vs 9.5M for B2)
- **Result**: MCC 0.3523 - FAILED (worse than B2)
- Conclusion: Larger model overfits on this dataset

### 9.2 DeepLabV3+ Architecture
**Script**: `bin_4/script_4_deeplabv3.py`
- Uses ASPP for multi-scale features
- **Result**: MCC 0.5246 - Not as good as FPN
- Conclusion: FPN architecture is better for this task

### 9.3 Hard Negative Mining ‚úÖ SUCCESS
**Script**: `bin_4/script_4_hard_negative_mining.py`
- Found 1328 hard negative images (56% of authentic produce FPs)
- Trained with 300 worst offenders
- **Result**: Dramatically reduced false positives when added to ensemble

---

## Phase 10: Recall Improvement Strategies (TODO)

Current best recall: 32% (balanced) or 62% (high-recall mode)
Goal: Improve recall while maintaining reasonable precision

### 10.1 Multi-Scale Inference + TTA ‚è±Ô∏è Quick
Run inference at multiple resolutions and combine:
```python
scales = [384, 512, 640]
# Union of detections across scales
# Add horizontal/vertical flip augmentation
```
**Expected**: +10-15% recall
**Tradeoff**: Slower inference (3x), may increase FP

### 10.2 Hard False-Negative Mining ‚è±Ô∏è ~1 hour training
Similar to hard negative mining, but for missed forgeries:
1. Identify the ~68 forgeries we're missing at threshold 0.75
2. Analyze why they're missed (small size? low contrast?)
3. Oversample these cases in training (3-5x weight)
4. Add copy-paste augmentation from these specific forgeries

**Expected**: +15-20% recall on hard cases

### 10.3 Small Forgery Specialist Model ‚è±Ô∏è ~1 hour
Train a separate model optimized for small forgeries (<2% area):
- Smaller input crops (256x256)
- More aggressive augmentation
- Higher weight for small forgery samples
- Combine with main model

**Expected**: Better detection of small manipulations

### 10.4 Lower Threshold with Confidence Filtering
Instead of single threshold, use two-stage approach:
```python
# Stage 1: Low threshold for candidates (0.5)
# Stage 2: Filter by region confidence and size
# Keep regions with: conf > 0.7 OR area > 1%
```
**Expected**: +10% recall with minimal FP increase

### 10.5 Test-Time Augmentation (TTA)
Apply augmentations at inference and average predictions:
```python
augmentations = [
    identity,
    horizontal_flip,
    vertical_flip,
    rotate_90,
    rotate_180,
    rotate_270
]
# Average all predictions
```
**Expected**: +5-10% recall, more stable predictions

### 10.6 Ensemble Voting Strategy
Instead of weighted average, use voting:
```python
# Require N out of 4 models to agree
# Lower N = higher recall, more FP
# N=2: ~70% recall, ~20% FP
# N=3: ~40% recall, ~5% FP
```

### 10.7 Train on Full Dataset with Augmentation
Current training uses ~2750 images. Options:
- Add augmented authentic images as negative samples
- Generate synthetic forgeries for training
- Use mixup/cutmix augmentation

---

## Phase 11: Data Improvements (TODO)

### 11.1 Balance training by forgery size
### 11.2 Semi-supervised Learning
- Use confident predictions on unlabeled data
- Pseudo-labeling strategy

---

## Phase 1: Quick Wins (No Retraining Required) ‚úÖ COMPLETED

### 1.1 Lower Classification Threshold ‚úÖ
**File**: `bin_4/script_6_test_forged.py`, `bin_4/script_6_test_authentic.py`, `bin_4/script_6_test_validation.py`

**Change**: `MAX_THRESHOLD = 0.2` ‚Üí `MAX_THRESHOLD = 0.1`

**Actual Results**:
- Recall: 66% ‚Üí 74% (+8%) ‚úÖ
- Specificity: 97% ‚Üí 87% (-10%)
- MCC: 0.663 ‚Üí 0.615

### 1.2 Multi-Scale Inference ‚úÖ
**File**: Created `bin_4/script_6_test_multiscale.py`

**Actual Results** (scales=[256, 384, 512]):
| Approach | TP | FN | TN | FP | Recall | Specificity | MCC |
|----------|----|----|----|----|--------|-------------|-----|
| Single-scale (256) @ 0.1 | 74 | 26 | 87 | 13 | 74% | 87% | 0.615 |
| Multi-scale MAX @ 0.1 | 95 | 5 | 15 | 85 | 95% | 15% | 0.167 |
| Smart (‚â•2 scales @ 0.3) | 78 | 22 | 50 | 50 | 78% | 50% | 0.292 |

**Conclusion**: Multi-scale MAX gives excellent recall (95%) but too many false positives.
The smart agreement approach is a compromise but still has high FP rate.
**Recommendation**: Use single-scale with threshold 0.1 for best balance (MCC 0.615).

---

## Phase 2: Training Improvements (New Model)

### 2.1 Error Level Analysis (ELA) Input Channel
**File**: Create `bin_4/script_4_ela_training.py`

**Implementation**:
```python
def compute_ela(image, quality=90):
    # Save at lower quality and compute difference
    encode_param = [int(cv2.IMWRITE_JPEG_QUALITY), quality]
    _, encoded = cv2.imencode('.jpg', image, encode_param)
    decoded = cv2.imdecode(encoded, cv2.IMREAD_COLOR)
    ela = cv2.absdiff(image, decoded)
    ela = cv2.cvtColor(ela, cv2.COLOR_BGR2GRAY)
    ela = (ela * 10).clip(0, 255)  # Amplify differences
    return ela

# Model input: 4 channels (RGB + ELA)
model = smp.FPN(
    encoder_name='timm-efficientnet-b2',
    in_channels=4,  # RGB + ELA
    classes=1,
)
```

**Rationale**: ELA highlights compression inconsistencies that occur when regions are copy-pasted from different sources.

**Expected Improvement**: +5-15% recall

### 2.2 Attention-Gated Architecture
**File**: Add to `bin_4/script_4_ela_training.py`

**Implementation**:
```python
class AttentionGate(nn.Module):
    def __init__(self, in_channels):
        super().__init__()
        self.conv = nn.Sequential(
            nn.Conv2d(in_channels, in_channels // 4, 1),
            nn.BatchNorm2d(in_channels // 4),
            nn.ReLU(inplace=True),
            nn.Conv2d(in_channels // 4, 1, 1),
            nn.Sigmoid()
        )
    
    def forward(self, x):
        attention = self.conv(x)
        return x * attention

# Add after FPN decoder
class AttentionFPN(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base = base_model
        self.attention = AttentionGate(1)
    
    def forward(self, x):
        out = self.base(x)
        return self.attention(out)
```

**Rationale**: Learns to focus on suspicious regions rather than treating all pixels equally.

**Expected Improvement**: +3-8% recall

### 2.3 Patch-Based Hard Example Mining
**File**: Add to `bin_4/script_4_ela_training.py`

**Implementation**:
```python
class HardExampleDataset(Dataset):
    def __init__(self, image_paths, mask_paths, patch_size=128):
        self.patches = []
        for img_path, mask_path in zip(image_paths, mask_paths):
            mask = np.load(mask_path)
            if mask.max() > 0:
                # Find forgery centroid
                y_coords, x_coords = np.where(mask > 0.5)
                cy, cx = y_coords.mean(), x_coords.mean()
                # Extract patch centered on forgery
                self.patches.append({
                    'image': img_path,
                    'mask': mask_path,
                    'center': (int(cy), int(cx)),
                    'size': patch_size
                })
    
    def __getitem__(self, idx):
        # Extract and augment patch around forgery
        ...
```

**Rationale**: Forces the model to learn fine-grained forgery features by training on zoomed-in patches.

**Expected Improvement**: +5-10% recall on small forgeries

---

## Phase 3: Combined Training Script

### 3.1 Create Comprehensive Training Script
**File**: `bin_4/script_4_comprehensive.py`

**Features**:
1. ‚úÖ Higher resolution (512x512)
2. ‚úÖ Focal loss with gamma=3.0
3. ‚úÖ Small forgery oversampling (3x)
4. ‚úÖ ELA as 4th input channel
5. ‚úÖ Attention gates after decoder
6. ‚úÖ Patch-based hard example mining (mixed batches)
7. ‚úÖ Multi-scale training augmentation

**Training Configuration**:
```python
IMG_SIZE = 512
BATCH_SIZE = 4
EPOCHS = 30
FOCAL_GAMMA = 3.0
ELA_QUALITY = 90
PATCH_SIZE = 128
HARD_EXAMPLE_RATIO = 0.3  # 30% of batch are hard patches
```

---

## Phase 4: Multi-Scale Test Script

### 4.1 Create Multi-Scale Inference Test
**File**: `bin_4/script_6_test_multiscale.py`

**Implementation**:
- Load comprehensive model
- Run inference at scales [256, 384, 512]
- MAX pool predictions across scales
- Use threshold 0.1 for classification

---

## Implementation Timeline

| Phase | Task | Time Estimate | Priority |
|-------|------|---------------|----------|
| 1.1 | Lower threshold | 5 minutes | High |
| 1.2 | Multi-scale inference | 30 minutes | High |
| 2.1-2.3 | Comprehensive training script | 2 hours | High |
| 3.1 | Run training | ~2 hours (GPU) | High |
| 4.1 | Multi-scale test script | 30 minutes | Medium |

---

## Expected Final Performance

| Metric | Current | Target |
|--------|---------|--------|
| Recall | 66% | 85-90% |
| Specificity | 97% | 85-90% |
| MCC | 0.663 | 0.75-0.80 |
| Accuracy | 81.5% | 85-88% |

---

## Phase 6: High-Resolution No-ELA Training (December 8, 2025)

### 6.1 Configuration
- **Resolution**: 512x512
- **Input**: 3-channel RGB (no ELA)
- **Architecture**: EfficientNet-B2 FPN + Attention Gate
- **Loss**: Focal Loss (gamma=3.0)
- **Epochs**: 30
- **Model**: `models/highres_no_ela_best.pth`

### 6.2 Results

| Thresh | Det | Auth FP | Recall | Precision | F1 | IoU | MCC |
|--------|-----|---------|--------|-----------|-----|-----|-----|
| 0.3 | 81 | 80/100 | 0.6024 | 0.5791 | 0.5905 | 0.4189 | 0.5780 |
| 0.5 | 76 | 67/100 | 0.5794 | 0.6221 | 0.6000 | 0.4286 | 0.5887 |
| 0.7 | 73 | 58/100 | 0.5506 | 0.6683 | 0.6038 | 0.4324 | 0.5959 |
| 0.9 | 69 | 49/100 | 0.5036 | 0.7680 | 0.6083 | 0.4371 | 0.6130 |

### 6.3 Comparison with Other Models

| Model | Thresh | Recall | Precision | F1 | MCC | Auth FP |
|-------|--------|--------|-----------|-----|-----|---------|
| **High-res No-ELA** | 0.5 | **0.5794** | 0.6221 | **0.6000** | **0.5887** | 67/100 |
| Comprehensive (ELA) | 0.5 | 0.4495 | 0.5416 | 0.4913 | 0.4795 | 78/100 |
| SAFIRE | 127 | 0.3476 | 0.0645 | 0.1089 | - | 99/100 |

### 6.4 Key Findings

1. **Removing ELA improved performance significantly**:
   - Recall: +13% (58% vs 45%)
   - F1: +11% (0.60 vs 0.49)
   - MCC: +23% (0.59 vs 0.48)

2. **Why ELA didn't help**:
   - Scientific images are PNG (lossless) - no JPEG artifacts to detect
   - ELA adds noise that confuses the model
   - RGB-only model focuses on actual visual forgery patterns

3. **Best operating point**: Threshold 0.9 gives MCC 0.6130 with 77% precision

---

## Validation Strategy

1. **Test on `test_forged_100`**: Measure recall improvement
2. **Test on `test_authentic_100`**: Ensure specificity doesn't drop too much
3. **Test on full `validation_images`**: 934 images for robust evaluation
4. **Analyze remaining misses**: Check if missed forgeries are even smaller or different type

---

## Files to Create

1. `bin_4/script_4_comprehensive.py` - Combined training with all improvements
2. `bin_4/script_6_test_multiscale.py` - Multi-scale inference testing
3. Update existing test scripts with threshold 0.1 option

---

## Notes

- **GPU Memory**: 512x512 with batch size 4 requires ~8GB VRAM
- **Training Time**: ~2 hours for 30 epochs with comprehensive model
- **Inference Time**: Multi-scale inference is 3x slower but acceptable for batch processing
- **Fallback**: If recall doesn't improve significantly, consider ensemble with current model

---

## Phase 5: SAFIRE (Segment Any Forged Image Region)

### 5.1 Overview
**Paper**: SAFIRE: Segment Any Forged Image Region (AAAI 2025)
**Authors**: Kwon et al.
**Repository**: https://github.com/mjkwon2021/SAFIRE

**Key Innovation**: Uses SAM (Segment Anything Model) with point prompting for forgery localization. Instead of classifying pixels directly, it uses a 16x16 grid of point prompts to guide SAM's segmentation toward forged regions.

### 5.2 Architecture
- **Base**: SAM ViT-B (Vision Transformer)
- **Adaptor Layers**: Frozen backbone with trainable adaptor layers
- **Parameters**: ~93M total (vs our 9.5M)
- **Input Resolution**: 1024x1024

### 5.3 Setup (Completed)
**Location**: `SAFIRE/`

**Weights Downloaded**:
| File | Size | Description |
|------|------|-------------|
| `sam_vit_b_01ec64.pth` | 375MB | SAM base weights |
| `safire.pth` | 421MB | SAFIRE fine-tuned (epoch 133) |
| `safire_encoder_pretrained.pth` | 1.08GB | Pretrained encoder |

### 5.4 Inference Testing
**Script**: `SAFIRE/infer_binary.py`

**Command**:
```bash
source ~/miniconda3/etc/profile.d/conda.sh && conda activate phi4
cd SAFIRE && python infer_binary.py
```

**Input**: `SAFIRE/ForensicsEval/inputs/` (copy test images here)
**Output**: `SAFIRE/ForensicsEval/outputs_binary/` (segmentation masks)

### 5.5 Performance Comparison

| Model | Params | Resolution | Inference Time | Training Data |
|-------|--------|------------|----------------|---------------|
| Our FPN | 9.5M | 512x512 | ~0.05s/image | Scientific images |
| SAFIRE | 93M | 1024x1024 | ~8s/image | General forgery datasets |

### 5.6 Training on Our Data (Future)

To fine-tune SAFIRE on scientific images:

1. **Create Dataset Class**: Extend `ForensicsEval/data/abstract.py`
```python
class Dataset_Scientific(AbstractForgeryDataset):
    def __init__(self):
        super().__init__()
        self.root_path = Path('/path/to/train_images')
        self.mask_path = Path('/path/to/train_masks')
        # ... populate im_list
```

2. **Training Command** (requires multi-GPU):
```bash
torchrun --nproc-per-node=1 train.py \
    --batch_size=2 \
    --encresume="safire_encoder_pretrained.pth" \
    --resume="" \
    --num_epochs=50
```

3. **Considerations**:
   - Multi-GPU recommended (original used 6 GPUs)
   - High memory requirements (~5GB per image)
   - Long training time (days vs hours)

### 5.7 Evaluation Results (December 8, 2025)

#### SAFIRE vs Our Model Comparison

| Model | Threshold | Forged Det | Auth FP | Recall | Precision | F1 | MCC |
|-------|-----------|------------|---------|--------|-----------|-----|-----|
| **Our Model** | 0.5 | 82/100 | 78/100 | 0.4495 | 0.5416 | 0.4913 | 0.4795 |
| **SAFIRE** | 127 (0.5) | 100/100 | 99/100 | 0.3476 | 0.0645 | 0.1089 | - |

#### Key Findings

1. **Our model significantly outperforms SAFIRE** on scientific images:
   - 8x higher precision (54% vs 6%)
   - Better F1 (0.49 vs 0.11)
   - Better recall (45% vs 35%)

2. **SAFIRE doesn't generalize well** to scientific images:
   - Trained on general forgery datasets (FantasticReality, CASIA, tampCOCO)
   - Different forgery patterns than scientific figures
   - Extremely high false positive rate (99/100 on authentic)

3. **Domain-specific training matters**:
   - Our model trained on scientific images with ELA features
   - SAFIRE's general forgery knowledge doesn't transfer well

4. **Both models have high FP rates** on authentic images:
   - Our model: 78/100 false positives at threshold 0.5
   - SAFIRE: 99/100 false positives
   - Suggests challenging task, models too sensitive

#### Our Model at Different Thresholds

| Thresh | Det | Auth FP | Recall | Precision | F1 | IoU | MCC |
|--------|-----|---------|--------|-----------|-----|-----|-----|
| 0.3 | 87 | 86/100 | 0.4761 | 0.5090 | 0.4920 | 0.3262 | 0.4774 |
| 0.5 | 82 | 78/100 | 0.4495 | 0.5416 | 0.4913 | 0.3256 | 0.4795 |
| 0.7 | 79 | 72/100 | 0.4133 | 0.5670 | 0.4781 | 0.3141 | 0.4709 |
| 0.9 | 75 | 67/100 | 0.3712 | 0.6123 | 0.4622 | 0.3006 | 0.4648 |

#### Conclusion

**SAFIRE is not suitable for scientific image forgery detection without fine-tuning.**
Our domain-specific model performs significantly better. Future work could explore:
- Fine-tuning SAFIRE on scientific images
- Ensemble of our model + SAFIRE
- Using SAFIRE's architecture with our training data

### 5.8 Current Status
- [x] Repository cloned
- [x] Weights downloaded (sam_vit_b, safire.pth, encoder_pretrained)
- [x] Inference on forged test images (100/100 complete)
- [x] Inference on authentic test images (100/100 complete)
- [x] Evaluate segmentation quality vs ground truth
- [x] Compare with our model on same test set
- [ ] Fine-tune on scientific images (optional - low priority given results)

---

## Phase 12: False Positive Reduction Model (December 10, 2025)

### 12.1 Goal
Reduce the high false positive rate (62%) on authentic images while maintaining reasonable recall on forged images.

### 12.2 Data Augmentation
**Script**: `bin_4/generate_authentic_augmented.py`

Generated 7,131 augmented authentic images with:
- Random rotations, flips, color jitter
- JPEG compression artifacts
- Gaussian noise/blur
- Contrast/brightness adjustments

Total authentic training images: 12,208 (original 5,077 + augmented 7,131)

### 12.3 FP Reduction Model v1 (FP_WEIGHT=3.0)
**Script**: `bin_4/script_7_fp_reduction.py`

**Configuration**:
- FP_WEIGHT: 3.0 (penalize false positives 3x more)
- Authentic ratio: 60% per batch
- Epochs: 20
- Model: `models/fp_reduction_best.pth`

**Training Results (Best at Epoch 9)**:
| Metric | Value |
|--------|-------|
| Specificity | 99.82% |
| FPR | 0.18% |
| Recall | **7.25%** |
| Precision | 11.68% |
| MCC | 0.0874 |

**Problem**: Model became too biased toward predicting "authentic" - barely detects any forgeries.

### 12.4 Veto Model Integration
**Script**: `bin_4/script_8_generate_submission.py`

Integrated FP reduction model as a "veto" that multiplicatively reduces predictions:
```python
veto_factor = 0.75 + 0.25 * fp_pred  # Range: 0.75 to 1.0
result = result * veto_factor
```

### 12.5 Results with Veto (December 10, 2025)

| Test Set | Without Veto | With Veto | Change |
|----------|-------------|-----------|--------|
| **Forged 100** | 85% detected | **39%** detected | **-46%** ‚ùå |
| **Authentic 100** | 62% FP | **2%** FP | **-60%** ‚úÖ |

**Analysis**: The veto dramatically reduced false positives but also killed recall. The FP reduction model is too aggressive - it predicts almost everything as "authentic".

### 12.6 Root Cause
The `FP_WEIGHT=3.0` made the model over-optimize for specificity at the expense of any forgery detection capability. After 20 epochs, it learned to say "authentic" for nearly every input.

### 12.7 Solution: Retrain with Lower FP_WEIGHT

**New Configuration**:
- FP_WEIGHT: **1.5** (down from 3.0)
- Balanced training with authentic_ratio: 55% (down from 60%)
- Target: Specificity ~90%, Recall ~50%

**Expected Outcome**:
| Test Set | Current | Target |
|----------|---------|--------|
| Forged 100 | 39% | 70-75% |
| Authentic 100 | 2% FP | 10-15% FP |

### 12.8 Status
- [x] Generated authentic augmented data (7,131 images)
- [x] Trained FP reduction model v1 (FP_WEIGHT=3.0)
- [x] Integrated veto into submission pipeline
- [x] Tested - identified over-aggressive veto problem
- [ ] **Retrain with FP_WEIGHT=1.5** ‚Üê NEXT STEP
- [ ] Re-test with balanced veto model
---

## Phase 13: Multi-Version Training (December 11-12, 2025) ‚úÖ COMPLETED

### 13.1 Training Pipeline Update
Updated `run_pipeline.sh` to support three training phases:
- **Phase 1a (v1)**: Base training - 7 models
- **Phase 1b (v2)**: Hard negative mining - 4 models fine-tuned with 73 FP-producing images
- **Phase 1c (v3)**: Augmented data training - 4 models trained with 10,315 forged images

### 13.2 V2 Hard Negative Mining
**Script**: `bin_4/script_8_hard_negative_v2.py`

- Identified 73 authentic images that consistently produce false positives
- Fine-tuned v1 models with 3x weight on hard negatives
- 25 epochs, ~5 hours total

**Models Created**:
- `highres_no_ela_v2_best.pth`
- `hard_negative_v2_best.pth`
- `high_recall_v2_best.pth`
- `enhanced_aug_v2_best.pth`

### 13.3 V3 Augmented Data Training
**Script**: `bin_4/script_9_train_with_augmented.py`

- Generated 7,564 augmented forged images (3 augmentations per original)
- Total forged: 10,315 images (was 2,751)
- 25 epochs, ~22 hours total

**Models Created**:
- `highres_no_ela_v3_best.pth`
- `hard_negative_v3_best.pth`
- `high_recall_v3_best.pth`
- `enhanced_aug_v3_best.pth`

### 13.4 Results Comparison (100 Forged + 100 Authentic Test Set)

| Version | Description | Forged Recall | Authentic FP | FP Reduction |
|---------|-------------|---------------|--------------|--------------|
| v1 | Original training | 85% | 73% | baseline |
| **v2** | Hard negative mining | **76%** | **62%** | **-11%** ‚úÖ |
| **v3** | Augmented forged data | **77%** | **62%** | **-11%** ‚úÖ |

### 13.5 Key Findings
1. **Both v2 and v3 achieved significant FP reduction** (-11 percentage points)
2. **Recall decreased by ~8-9%** (85% ‚Üí 76-77%) - acceptable trade-off
3. **v2 and v3 perform similarly** - different approaches, same result
4. **Hard negatives are effective** - targeting FP-producing images directly works

### 13.6 Recommended Configuration
```bash
# Use v3 models (auto-detected as best available)
python script_10_generate_submission.py -i <test_dir> -o submission.csv --model-version v3
```

### 13.7 Next Steps
- [x] Test combining v2 and v3 models in ensemble
- [x] Threshold optimization with new models

---

## Phase 14: Combined v2+v3 Ensemble (December 12, 2025) ‚úÖ COMPLETED

### 14.1 Combined Ensemble Configuration
Created 10-model ensemble combining:
- 4 v2 models (hard negative mining - better at avoiding FPs)
- 4 v3 models (augmented data - trained on more forged examples)
- 1 comprehensive model (ELA-based)

### 14.2 Threshold Sweep Results (v2+v3, 10 models)

| Threshold | Forged Recall | Authentic FP | Net (Recall - FP) |
|-----------|---------------|--------------|-------------------|
| 0.50 | 80% | 67% | +13% |
| 0.55 | 80% | 66% | +14% |
| 0.60 | 78% | 64% | +14% |
| 0.65 | 76% | 62% | +14% |
| **0.70** | **75%** | **58%** | **+17%** ‚≠ê |
| **0.75** | **70%** | **53%** | **+17%** ‚≠ê |

### 14.3 Comparison with Baseline

| Configuration | Forged Recall | Authentic FP | FP Reduction |
|---------------|---------------|--------------|--------------|
| v1 @ t=0.65 (baseline) | 85% | 73% | - |
| v2 @ t=0.65 | 76% | 62% | -11% |
| v3 @ t=0.65 | 77% | 62% | -11% |
| v2+v3 @ t=0.65 | 76% | 62% | -11% |
| **v2+v3 @ t=0.75** | **70%** | **53%** | **-20%** ‚úÖ |

### 14.4 Key Findings
1. **v2+v3 @ t=0.65 same as individual v2/v3** - models learned similar patterns
2. **Higher threshold (0.75) provides best trade-off** - 20% FP reduction vs 15% recall loss
3. **Trade-off ratio: 0.75** - losing 0.75% recall per 1% FP reduction (good!)

### 14.5 Best Configuration
```bash
# Recommended: v2+v3 combined with threshold 0.75
python script_10_generate_submission.py -i <test_dir> -o submission.csv --model-version v2+v3 --threshold 0.75
```

**Expected Performance:**
- Forged Recall: 70%
- Authentic FP: 53%
- Models: 10 (4 v2 + 4 v3 + 1 comprehensive)
- [ ] Submit to Kaggle
---

## Phase 15: False Positive Image Analysis (December 12, 2025) ‚úÖ COMPLETED

### 15.1 Objective
Analyze the 62 remaining false positive images to understand *why* they're being misclassified and identify potential mitigation strategies.

### 15.2 Detection Size Analysis

Of 62 FP images at t=0.65:

| Detection Size | Count | Cumulative % | Min-Area Filter Effect |
|----------------|-------|--------------|------------------------|
| < 100 pixels | 3 | 4.8% | min-area=100 eliminates |
| < 500 pixels | 27 | 43.5% | min-area=500 eliminates |
| < 1000 pixels | 37 | 59.7% | min-area=1000 eliminates |
| < 5000 pixels | 51 | 82.3% | min-area=5000 eliminates |
| ‚â• 5000 pixels | **11** | **17.7%** | **Hard cases** |

**Key Finding**: ~43% of FPs can be eliminated with min-area ‚â• 500 pixels.

### 15.3 Image Property Analysis

| Category | Count | % of FPs | Description |
|----------|-------|----------|-------------|
| Dark + Low Contrast + Small | 14 | 22.6% | Nearly black, small images |
| Small only | 13 | 21.0% | Small images (<500k pixels) |
| Low Contrast + Small | 13 | 21.0% | Uniform regions, small |
| Dark + Low Contrast | 11 | 17.7% | Nearly black, large |
| Normal | 8 | 12.9% | Regular photographs |
| Large | 2 | 3.2% | Large textured images |
| Low Contrast only | 1 | 1.6% | Uniform but medium-sized |

**Summary Statistics:**
- **40% are dark images** (mean brightness < 30/255)
- **63% are low contrast** (std < 30)
- **Only 13% are "normal"** images

### 15.4 Hardest Cases (Detection > 5000 pixels)

| Case ID | Detection | Mean | Std | Category |
|---------|-----------|------|-----|----------|
| 12244 | 162,789 px | 153 | 42 | Large normal image |
| 10138 | 47,181 px | 164 | 51 | Large normal image |
| 10147 | 33,974 px | 61 | 23 | Low contrast grayscale |
| 11399 | 16,620 px | **1.7** | 3.8 | **Very dark** |
| 11277 | 9,582 px | **3.5** | 5.2 | **Very dark** |
| 11446 | 9,166 px | **4.8** | 8.1 | **Very dark** |
| 12046 | 8,669 px | **4.5** | 5.7 | **Very dark** |
| 10015 | 7,142 px | **7.3** | 8.1 | **Very dark** |
| 10435 | 6,344 px | 192 | 40 | Small normal |
| 11854 | 5,608 px | **1.3** | 5.3 | **Very dark** |
| 11223 | 5,518 px | **4.9** | 6.8 | **Very dark** |

**Key Finding**: 7 of 11 hardest cases are **nearly black images** (astronomy, microscopy, etc.)

### 15.5 Root Causes

1. **Dark Images (40% of FPs)**
   - Nearly black scientific images (microscopy, astronomy, electrophoresis gels)
   - Model detects ELA/texture artifacts in uniform dark regions
   - Mean pixel value often < 10/255

2. **Low Contrast Images (63% of FPs)**
   - Uniform regions (solid colors, gradients)
   - Subtle texture differences interpreted as forgery
   - Charts, diagrams, simple graphics

3. **Texture Artifacts in Normal Photos**
   - Natural textures (foliage, fabric, skin) trigger detections
   - Compression artifacts on borders
   - Only 13% of FPs but hardest to filter

### 15.6 Proposed Solutions

| Solution | FP Reduction | Implementation | Recall Impact |
|----------|--------------|----------------|---------------|
| **min-area=1000** | ~60% | Post-processing | Moderate |
| **Dark image filter** | ~40% | Skip if mean < 30 | None |
| **Contrast filter** | ~63% | Skip if std < 30 | Low |
| **Higher threshold** | Variable | Already at 0.75 | High |
| **Retrain on dark images** | Unknown | Add to hard negatives | Unknown |

### 15.7 Recommended Next Steps

1. **Quick Win: Increase min-area to 1000**
   - Eliminates 60% of FPs
   - Some recall loss on small forgeries
   - Easy to implement

2. **Dark Image Pre-filter**
   - Skip detection if image mean < 30
   - Eliminates 40% of FPs
   - Zero impact on typical images

3. **Add Dark Images to Hard Negatives**
   - Fine-tune v4 models on problematic dark/low-contrast images
   - May teach model to ignore these patterns

4. **Confidence-Based Filtering**
   - For low-contrast images, require higher confidence
   - Adaptive threshold based on image properties

---

## Phase 16: Next Actions (TODO)

### Option 1: Test min-area=1000 with v2+v3 Ensemble
```bash
python script_10_generate_submission.py -i <test_dir> -o submission.csv \
    --model-version v2+v3 --threshold 0.75 --min-area 1000
```
**Expected**: ~60% FP reduction, some recall loss on small forgeries

### Option 2: Implement Dark Image Pre-filter
Add to submission script:
```python
def should_skip_image(image):
    """Skip detection for very dark images that cause false positives."""
    mean_brightness = np.mean(image)
    return mean_brightness < 30  # Nearly black images
```
**Expected**: ~40% FP reduction on hardest cases, zero recall impact

### Option 3: Prepare v4 Training with Dark Hard Negatives
1. Collect 25 dark FP images (mean < 30)
2. Add to hard negative training set
3. Fine-tune v4 models (4 models, ~5 hours)
```bash
# Add dark images to hard negatives
python script_8_hard_negative_v2.py --version v4 --add-dark-negatives
```
**Expected**: Model learns to ignore dark image artifacts

### Priority Order
1. ‚è±Ô∏è **Option 1** (5 min) - Quick test, immediate results
2. ‚è±Ô∏è **Option 2** (30 min) - Code change, good FP reduction
3. ‚è±Ô∏è **Option 3** (5+ hours) - Retraining, best long-term solution

---

## Phase 17: Min-Area Filter Results (December 12, 2025) ‚úÖ COMPLETED

### 17.1 Test Results (v2+v3 ensemble, t=0.75)

| min-area | Forged Recall | Authentic FP | FP Reduction | Trade-off Ratio |
|----------|---------------|--------------|--------------|-----------------|
| 50 (baseline) | 70% | 53% | - | - |
| **500** ‚≠ê | **62%** | **20%** | **-33%** | **4.1** |
| 1000 | 52% | 12% | -41% | 2.3 |

### 17.2 Best Configuration So Far
```bash
python script_10_generate_submission.py -i <test_dir> -o submission.csv \
    --model-version v2+v3 --threshold 0.75 --min-area 500
```

**Performance:**
- Forged Recall: **62%**
- Authentic FP: **20%**
- Trade-off ratio: 4.1 (lose 0.24% recall per 1% FP reduction - excellent!)

### 17.3 Analysis
- min-area=500 provides best balance of recall vs FP reduction
- 33% FP reduction with only 8% recall loss
- Trade-off ratio of 4.1 is much better than min-area=1000 (2.3)

---

## Phase 18: Final Threshold Optimization (December 12, 2025) ‚úÖ COMPLETED

### 18.1 Threshold Sweep with min-area=500 (v2+v3 ensemble)

| Threshold | Forged Recall | Authentic FP | Net (Recall - FP) |
|-----------|---------------|--------------|-------------------|
| 0.65 | 66% | 28% | +38 |
| 0.70 | 64% | 26% | +38 |
| **0.75** | **62%** | **20%** | **+42** ‚≠ê |
| 0.80 | 38% | 0% | +38 |
| 0.85 | 38% | 0% | +38 |

### 18.2 Key Findings
1. **t=0.75 has best net score (+42)** - optimal balance
2. Sharp cliff between t=0.75 and t=0.80 (recall drops 24%)
3. t=0.80+ achieves 0% FP but sacrifices too much recall

### 18.3 Dark Image Filter Results (Option 2)
Tested but **not recommended**:
- 31% of forged images are also dark
- Skipping dark images: 45% recall, 16% FP (loses 17% recall)
- Adaptive min-area for dark: 50% recall, 16% FP (not worth it)

### 18.4 üèÜ FINAL BEST CONFIGURATION

```bash
python script_10_generate_submission.py -i <test_dir> -o submission.csv \
    --model-version v2+v3 --threshold 0.75 --min-area 500
```

| Metric | Value |
|--------|-------|
| **Forged Recall** | **62%** |
| **Authentic FP** | **20%** |
| **Net Score** | **+42** |
| Models | 10 (4 v2 + 4 v3 + 1 comprehensive + 1 specialist) |
| Threshold | 0.75 |
| Min-Area | 500 pixels |

---

## Phase 19: V4 Models + TTA (December 13, 2025) ‚úÖ COMPLETED

### 19.1 V4 Training
Trained 4 new models with hard negative mining (677 FP-producing authentic images) + FP penalty loss:
- `highres_no_ela_v4`
- `hard_negative_v4`
- `high_recall_v4`
- `enhanced_aug_v4`

Training time: 4.8 hours

### 19.2 V4 vs V2+V3 Comparison

| Ensemble | Models | Recall | FP | Net |
|----------|--------|--------|-----|-----|
| **v4 only** | **4** | **63%** | **4%** | **+59** ‚≠ê |
| v2+v3+v4 | 12 | 62% | 4% | +58 |
| v2+v3 | 8 | 60% | 7% | +53 |

**V4 dramatically reduces FP** (4% vs 20% for v2+v3) while maintaining recall.

### 19.3 Test-Time Augmentation (TTA)

Added TTA with max aggregation (flip horizontal, vertical, both):

| Mode | Forged Recall | FP Rate | Net |
|------|---------------|---------|-----|
| No TTA | 68% | 4% | +64 |
| TTA (mean) | 65% | 1% | +64 |
| **TTA (max)** | **74%** | **7%** | **+67** ‚≠ê |

### 19.4 üèÜ NEW BEST CONFIGURATION

```bash
python script_2_generate_submission.py -i <test_dir> -o submission.csv --tta
```

| Metric | Value |
|--------|-------|
| **Forged Recall** | **74%** |
| **Authentic FP** | **7%** |
| **Net Score** | **+67** |
| Models | 4 (v4 ensemble) |
| Threshold | 0.20 |
| Min-Area | 500 pixels |
| TTA | max aggregation (4x: original + flips) |

**+25 net score improvement over previous best (+42 ‚Üí +67)!**

---

## Phase 20: Adaptive Thresholding + Post-Processing (December 13, 2025) ‚úÖ COMPLETED

### 20.1 Missed Forgery Analysis

Analyzed patterns in missed forgeries (300 images):
- **Dark images**: 53% of missed vs 29% of detected (brightness < 50)
- **Small forgeries**: Mean 8,783 px for missed vs 54,723 px for detected

### 20.2 Adaptive Thresholding for Dark Images

Lower threshold and min_area for dark images where forgeries are harder to detect:

| Image Brightness | Threshold Adjustment | Min-Area Adjustment |
|-----------------|---------------------|---------------------|
| < 50 (very dark) | -0.08 (e.g., 0.30 ‚Üí 0.22) | √∑2.5 (e.g., 500 ‚Üí 200) |
| 50-80 (moderately dark) | -0.04 (e.g., 0.30 ‚Üí 0.26) | √∑1.5 (e.g., 500 ‚Üí 333) |
| ‚â•80 (normal) | No change | No change |

**Results** (400 images):

| Mode | Recall | FP | Net |
|------|--------|-----|-----|
| Fixed threshold | 70.0% | 4.2% | +263 |
| **Adaptive** | **77.8%** | **7.0%** | **+283** |

Dark image recall improved: **58% ‚Üí 81%** (+23%)

### 20.3 Confidence Filtering

Reject detections where max confidence in the detected region is below threshold:

| min_confidence | Recall | FP | Net |
|----------------|--------|-----|-----|
| 0.00 (disabled) | 77.8% | 7.0% | +283 |
| 0.42 | 77.0% | 5.2% | +287 |
| 0.44 | 77.0% | 5.0% | +288 |
| **0.45** | **77.0%** | **5.0%** | **+288** |
| 0.48 | 77.0% | 4.8% | +289 |
| 0.50 | 75.5% | 3.5% | +288 |

Selected **0.45** as default (good balance).

### 20.4 8x TTA (Tested, Not Default)

Added 90¬∞/270¬∞ rotations to TTA:

| TTA Mode | Recall | FP | Net |
|----------|--------|-----|-----|
| 4x (flips) | 77.0% | 3.0% | +74 |
| 8x (flips + rotations) | 79.0% | 5.0% | +74 |

8x TTA gains +2% recall but +2% FP - no net improvement. Kept as optional (`--tta-mode 8x`).

### 20.5 Morphological Cleanup

Applied morphological opening/closing to remove noise:

| Config | Recall | FP | Net |
|--------|--------|-----|-----|
| No morphology | 77.0% | 3.0% | +74 |
| **With morphology** | **78.0%** | **3.0%** | **+75** |

Small improvement - enabled by default.

### 20.6 üèÜ NEW BEST CONFIGURATION (Full Dataset)

```bash
python script_2_generate_submission.py -i <test_dir> -o submission.csv
```

Default settings now include all optimizations:

| Parameter | Value | Description |
|-----------|-------|-------------|
| threshold | 0.30 | Base detection threshold |
| min_area | 500 | Minimum forgery area (pixels) |
| TTA | 4x (default) | Flip augmentations |
| adaptive | True | Lower threshold for dark images |
| min_confidence | 0.45 | Reject low-confidence detections |
| morphological | True | Noise cleanup |

**Full Dataset Results (2751 forged, 2377 authentic):**

| Metric | Value |
|--------|-------|
| **Forged Recall** | **79.9%** (2198/2751) |
| **Authentic FP** | **7.3%** (174/2377) |
| **Net Score** | **+2024** |

### 20.7 Improvement Summary

| Phase | Net Score | Improvement |
|-------|-----------|-------------|
| Phase 18 (v2+v3) | +42 | Baseline |
| Phase 19 (v4 + TTA) | +67 | +25 |
| **Phase 20 (Adaptive + Confidence + Morph)** | **+2024** | **Full dataset** |

**Per 100 images: ~+72 net** (vs +67 in Phase 19)

### 20.8 CLI Options

```bash
# Default (best config)
python script_2_generate_submission.py -i test/ -o submission.csv

# Disable specific features
python script_2_generate_submission.py -i test/ -o submission.csv --no-adaptive
python script_2_generate_submission.py -i test/ -o submission.csv --min-confidence 0
python script_2_generate_submission.py -i test/ -o submission.csv --no-tta

# Use 8x TTA (slower, no net improvement)
python script_2_generate_submission.py -i test/ -o submission.csv --tta-mode 8x

# High precision mode
python script_2_generate_submission.py -i test/ -o submission.csv --mode high-precision
```
