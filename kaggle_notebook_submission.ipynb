{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8baa0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import timm\n",
    "import segmentation_models_pytorch as smp\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Configuration\n",
    "CLASSIFIER_THRESHOLD = 0.25\n",
    "SEG_THRESHOLD = 0.35\n",
    "MIN_AREA = 300\n",
    "CLASSIFIER_SIZE = 384\n",
    "SEG_SIZE = 512\n",
    "USE_TTA = True\n",
    "USE_ADAPTIVE = True\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "000767d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Definitions\n",
    "\n",
    "class ForgeryClassifier(nn.Module):\n",
    "    \"\"\"Binary classifier: forged (1) vs authentic (0).\"\"\"\n",
    "    def __init__(self, backbone='efficientnet_b2'):\n",
    "        super().__init__()\n",
    "        self.backbone = timm.create_model(backbone, pretrained=False, num_classes=0)\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.backbone.num_features, 256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features).squeeze(-1)\n",
    "\n",
    "\n",
    "class AttentionGate(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        hidden1 = max(8, in_channels // 2)\n",
    "        hidden2 = max(4, in_channels // 4)\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, hidden1, kernel_size=1),\n",
    "            nn.BatchNorm2d(hidden1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden1, hidden2, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(hidden2, 1, kernel_size=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x * self.conv(x)\n",
    "\n",
    "\n",
    "class AttentionFPN(nn.Module):\n",
    "    def __init__(self, base_model):\n",
    "        super().__init__()\n",
    "        self.base = base_model\n",
    "        self.attention = AttentionGate(1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.attention(self.base(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea67f787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Models\n",
    "# NOTE: Update these paths to your Kaggle dataset paths\n",
    "\n",
    "MODEL_DIR = Path('/kaggle/input/your-models-dataset')  # Update this!\n",
    "\n",
    "# Load binary classifier\n",
    "classifier = ForgeryClassifier('efficientnet_b2').to(device)\n",
    "classifier_ckpt = torch.load(MODEL_DIR / 'binary_classifier_best.pth', map_location=device)\n",
    "classifier.load_state_dict(classifier_ckpt['model_state_dict'])\n",
    "classifier.eval()\n",
    "print(\"✓ Binary classifier loaded\")\n",
    "\n",
    "# Load 4-model ensemble\n",
    "model_names = [\n",
    "    'highres_no_ela_v4_best.pth',\n",
    "    'hard_negative_v4_best.pth',\n",
    "    'high_recall_v4_best.pth',\n",
    "    'enhanced_aug_v4_best.pth'\n",
    "]\n",
    "\n",
    "seg_models = []\n",
    "for name in model_names:\n",
    "    base = smp.FPN(\n",
    "        encoder_name=\"timm-efficientnet-b2\",\n",
    "        encoder_weights=None,\n",
    "        in_channels=3,\n",
    "        classes=1,\n",
    "    )\n",
    "    model = AttentionFPN(base).to(device)\n",
    "    ckpt = torch.load(MODEL_DIR / name, map_location=device)\n",
    "    if 'model_state_dict' in ckpt:\n",
    "        model.load_state_dict(ckpt['model_state_dict'])\n",
    "    else:\n",
    "        model.load_state_dict(ckpt)\n",
    "    model.eval()\n",
    "    seg_models.append(model)\n",
    "    print(f\"  ✓ {name}\")\n",
    "\n",
    "print(f\"✓ Loaded {len(seg_models)} ensemble models\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0ba6965",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocessing functions\n",
    "\n",
    "def preprocess_for_classifier(img):\n",
    "    img_resized = cv2.resize(img, (CLASSIFIER_SIZE, CLASSIFIER_SIZE))\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    img_norm = img_rgb.astype(np.float32) / 255.0\n",
    "    img_norm = (img_norm - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    tensor = torch.from_numpy(img_norm.transpose(2, 0, 1)).unsqueeze(0).float().to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def preprocess_for_segmentation(img):\n",
    "    img_resized = cv2.resize(img, (SEG_SIZE, SEG_SIZE))\n",
    "    img_rgb = cv2.cvtColor(img_resized, cv2.COLOR_BGR2RGB)\n",
    "    img_norm = img_rgb.astype(np.float32) / 255.0\n",
    "    img_norm = (img_norm - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])\n",
    "    tensor = torch.from_numpy(img_norm.transpose(2, 0, 1)).unsqueeze(0).float().to(device)\n",
    "    return tensor\n",
    "\n",
    "\n",
    "def apply_tta_ensemble(models, img_tensor):\n",
    "    \"\"\"Apply 4x TTA with mean aggregation across ensemble.\"\"\"\n",
    "    all_preds = []\n",
    "    \n",
    "    for model in models:\n",
    "        preds = []\n",
    "        \n",
    "        # Original\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(model(img_tensor))\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Horizontal flip\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(model(torch.flip(img_tensor, dims=[3])))\n",
    "            pred = torch.flip(pred, dims=[3])\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Vertical flip\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(model(torch.flip(img_tensor, dims=[2])))\n",
    "            pred = torch.flip(pred, dims=[2])\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Both flips\n",
    "        with torch.no_grad():\n",
    "            pred = torch.sigmoid(model(torch.flip(img_tensor, dims=[2, 3])))\n",
    "            pred = torch.flip(pred, dims=[2, 3])\n",
    "        preds.append(pred)\n",
    "        \n",
    "        # Mean for this model's TTA\n",
    "        model_pred = torch.stack(preds).mean(dim=0)\n",
    "        all_preds.append(model_pred)\n",
    "    \n",
    "    # Mean across ensemble\n",
    "    return torch.stack(all_preds).mean(dim=0)\n",
    "\n",
    "\n",
    "def get_adaptive_threshold(img, base_threshold=0.35):\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    brightness = np.mean(gray) / 255.0\n",
    "    if brightness < 0.3:\n",
    "        return base_threshold * 0.85\n",
    "    elif brightness > 0.7:\n",
    "        return base_threshold * 1.15\n",
    "    return base_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2dbd257",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mask_to_rle(mask):\n",
    "    \"\"\"Convert binary mask to RLE string.\"\"\"\n",
    "    pixels = mask.flatten()\n",
    "    runs = []\n",
    "    prev = 0\n",
    "    start = 0\n",
    "    \n",
    "    for i, p in enumerate(pixels):\n",
    "        if p != prev:\n",
    "            runs.append(i - start)\n",
    "            start = i\n",
    "            prev = p\n",
    "    runs.append(len(pixels) - start)\n",
    "    \n",
    "    if pixels[0] == 1:\n",
    "        runs = [0] + runs\n",
    "    \n",
    "    rle_pairs = []\n",
    "    for i in range(0, len(runs) - 1, 2):\n",
    "        rle_pairs.append(f\"{runs[i]} {runs[i+1]}\")\n",
    "    \n",
    "    return ' '.join(rle_pairs) if rle_pairs else ''\n",
    "\n",
    "\n",
    "def process_image(img_path, classifier, seg_models):\n",
    "    \"\"\"Two-stage processing of a single image.\"\"\"\n",
    "    img = cv2.imread(str(img_path))\n",
    "    if img is None:\n",
    "        return 'authentic'\n",
    "    \n",
    "    orig_h, orig_w = img.shape[:2]\n",
    "    \n",
    "    # Stage 1: Binary classification\n",
    "    cls_tensor = preprocess_for_classifier(img)\n",
    "    with torch.no_grad():\n",
    "        cls_prob = torch.sigmoid(classifier(cls_tensor)).item()\n",
    "    \n",
    "    if cls_prob < CLASSIFIER_THRESHOLD:\n",
    "        return 'authentic'\n",
    "    \n",
    "    # Stage 2: Segmentation with ensemble + TTA\n",
    "    seg_tensor = preprocess_for_segmentation(img)\n",
    "    \n",
    "    if USE_TTA:\n",
    "        pred = apply_tta_ensemble(seg_models, seg_tensor)\n",
    "    else:\n",
    "        preds = []\n",
    "        for model in seg_models:\n",
    "            with torch.no_grad():\n",
    "                preds.append(torch.sigmoid(model(seg_tensor)))\n",
    "        pred = torch.stack(preds).mean(dim=0)\n",
    "    \n",
    "    pred_np = pred.squeeze().cpu().numpy()\n",
    "    \n",
    "    # Apply threshold\n",
    "    if USE_ADAPTIVE:\n",
    "        threshold = get_adaptive_threshold(img, SEG_THRESHOLD)\n",
    "    else:\n",
    "        threshold = SEG_THRESHOLD\n",
    "    \n",
    "    mask = (pred_np > threshold).astype(np.uint8)\n",
    "    \n",
    "    # Filter small regions\n",
    "    if MIN_AREA > 0:\n",
    "        num_labels, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "        mask_filtered = np.zeros_like(mask)\n",
    "        for i in range(1, num_labels):\n",
    "            if stats[i, cv2.CC_STAT_AREA] >= MIN_AREA:\n",
    "                mask_filtered[labels == i] = 1\n",
    "        mask = mask_filtered\n",
    "    \n",
    "    # Resize to original size\n",
    "    mask_orig = cv2.resize(mask, (orig_w, orig_h), interpolation=cv2.INTER_NEAREST)\n",
    "    \n",
    "    if mask_orig.sum() == 0:\n",
    "        return 'authentic'\n",
    "    \n",
    "    return mask_to_rle(mask_orig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18589abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process test images and generate submission\n",
    "\n",
    "TEST_DIR = Path('/kaggle/input/recodai-luc-scientific-image-forgery-detection/validation_images')\n",
    "\n",
    "image_files = sorted(TEST_DIR.glob('*'))\n",
    "print(f\"Found {len(image_files)} test images\")\n",
    "\n",
    "results = []\n",
    "for img_path in tqdm(image_files, desc=\"Processing\"):\n",
    "    case_id = img_path.stem\n",
    "    annotation = process_image(img_path, classifier, seg_models)\n",
    "    results.append({'case_id': case_id, 'annotation': annotation})\n",
    "\n",
    "# Create submission\n",
    "submission = pd.DataFrame(results)\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "print(f\"\\nSubmission saved!\")\n",
    "print(f\"Total: {len(submission)}\")\n",
    "print(f\"Forged: {(submission['annotation'] != 'authentic').sum()}\")\n",
    "print(f\"Authentic: {(submission['annotation'] == 'authentic').sum()}\")\n",
    "print(submission.head(10))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
